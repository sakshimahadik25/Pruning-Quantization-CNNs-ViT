{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.quantization\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import timm\n",
    "import copy\n",
    "import os\n",
    "import tempfile\n",
    "from torch.ao.quantization import get_default_qconfig, prepare, convert\n",
    "from torch.ao.quantization.observer import MinMaxObserver, PerChannelMinMaxObserver\n",
    "from torch.ao.quantization.qconfig import QConfig\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/msambare/fer2013?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60.3M/60.3M [00:00<00:00, 90.8MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/smahadi/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"msambare/fer2013\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 22968\n",
      "Validation samples: 5741\n",
      "Test samples: 7178\n"
     ]
    }
   ],
   "source": [
    "# Dataset loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "dataset = ImageFolder('fer2013/versions/1/train', transform=transform)\n",
    "val_size = int(0.2 * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "test_dataset = ImageFolder('fer2013/versions/1/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Check distribution\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizableResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): QuantizableBasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): QuantizableBasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): QuantizableBasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): QuantizableBasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): QuantizableBasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): QuantizableBasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): QuantizableBasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): QuantizableBasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=7, bias=True)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.quantization import resnet18 as resnet18_model\n",
    "\n",
    "resnet18 = resnet18_model(pretrained=True, quantize=False)\n",
    "resnet18.fc = nn.Linear(512, 7)\n",
    "resnet18.eval()\n",
    "resnet18.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstructured Pruning for CNN\n",
    "def unstructured_prune_cnn(model, amount=0.3):\n",
    "    model = copy.deepcopy(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
    "            prune.remove(module, \"weight\")\n",
    "    return model\n",
    "\n",
    "# Structured Pruning for CNN\n",
    "def structured_prune_cnn(model, amount=0.5):\n",
    "    model = copy.deepcopy(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            prune.ln_structured(module, name=\"weight\", amount=amount, n=2, dim=0)\n",
    "            prune.remove(module, \"weight\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_model_blocks(model):\n",
    "    torch.quantization.fuse_modules(model, [[\"conv1\", \"bn1\", \"relu\"]], inplace=True)\n",
    "    for module_name, module in model.named_children():\n",
    "        if \"layer\" in module_name:\n",
    "            for block in module:\n",
    "                torch.quantization.fuse_modules(\n",
    "                    block, [[\"conv1\", \"bn1\", \"relu\"], [\"conv2\", \"bn2\"]],\n",
    "                    inplace=True\n",
    "                )\n",
    "                if hasattr(block, \"downsample\") and isinstance(block.downsample, torch.nn.Sequential):\n",
    "                    if len(block.downsample) >= 2:\n",
    "                        torch.quantization.fuse_modules(block.downsample, [\"0\", \"1\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_trained_pruned_model(model, calibration_loader, num_batches=10):\n",
    "    import copy\n",
    "    model = copy.deepcopy(model)\n",
    "    model.cpu().eval()\n",
    "\n",
    "    # Fuse layers (must happen after pruning and training)\n",
    "    fuse_model_blocks(model)\n",
    "\n",
    "    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "    torch.quantization.prepare(model, inplace=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (x, _) in enumerate(calibration_loader):\n",
    "            x = x.to(torch.float32)\n",
    "            x = x * 0.5 + 0.5  # Undo normalization: [0, 1]\n",
    "            x = torch.clamp(x, 0.0, 1.0)\n",
    "            model(x)\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "\n",
    "    torch.quantization.convert(model, inplace=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model, calibration_loader, num_batches=10):\n",
    "    import copy\n",
    "    model = copy.deepcopy(model)\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "\n",
    "    # Fuse layers\n",
    "    fuse_model_blocks(model)\n",
    "\n",
    "    # Set quantization config and prepare\n",
    "    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "    torch.quantization.prepare(model, inplace=True)\n",
    "\n",
    "    # Calibration loop\n",
    "    with torch.no_grad():\n",
    "        for i, (x, _) in enumerate(calibration_loader):\n",
    "            x = x.to(torch.float32)\n",
    "            x = x * 0.5 + 0.5  # Undo Normalize([0.5], [0.5])\n",
    "            x = torch.clamp(x, 0.0, 1.0)  # Ensure values in [0, 1]\n",
    "            model(x)\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "\n",
    "    # Convert to quantized model\n",
    "    torch.quantization.convert(model, inplace=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs=10, lr=1e-4):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss += loss.item()\n",
    "                correct += (outputs.argmax(1) == y).sum().item()\n",
    "                total += y.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100 * correct / total\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, quantization=False):\n",
    "    model.eval()\n",
    "\n",
    "    if quantization:\n",
    "        model = model.to(\"cpu\")  # Quantized models must be on CPU\n",
    "    else:\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            if quantization:\n",
    "                x, y = x.to(\"cpu\"), y.to(\"cpu\")\n",
    "            else:\n",
    "                x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "\n",
    "            outputs = model(x)\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy = {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_speed(model, test_loader, quantization=False):\n",
    "    model.eval()\n",
    "    \n",
    "    if quantization:\n",
    "        device = \"cpu\"\n",
    "        \n",
    "    else:\n",
    "        device = torch.device(\"cuda\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for x, _ in test_loader:\n",
    "            x = x.to(device)\n",
    "            _ = model(x)\n",
    "    end = time.time()\n",
    "    latency = (end - start) / len(test_loader)\n",
    "    print(f\"Avg Inference Time per Batch: {latency:.4f} sec\")\n",
    "    return latency\n",
    "\n",
    "def model_size_mb(model, use_state_dict=True):\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "        if use_state_dict:\n",
    "            torch.save(model.state_dict(), f.name)\n",
    "        else:\n",
    "            torch.save(model, f.name)\n",
    "        size_mb = os.path.getsize(f.name) / (1024 * 1024)\n",
    "    print(f\"Model Size ({'state_dict' if use_state_dict else 'full model'}): {size_mb:.2f} MB\")\n",
    "    return size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Train baseline ===\n",
      "Epoch 1: Train Loss = 1.1643, Val Loss = 1.0048, Val Acc = 61.99%\n",
      "Test Accuracy = 62.58%\n",
      "Model Size (state_dict): 42.73 MB\n",
      "Avg Inference Time per Batch: 0.1355 sec\n",
      "=== Apply structured pruning, then fine-tune ===\n",
      "Epoch 1: Train Loss = 1.1940, Val Loss = 1.0761, Val Acc = 59.05%\n",
      "Test Accuracy = 58.58%\n",
      "Model Size (state_dict): 42.73 MB\n",
      "Avg Inference Time per Batch: 0.1286 sec\n",
      "=== Apply unstructured pruning, then fine-tune ===\n",
      "Epoch 1: Train Loss = 0.8098, Val Loss = 0.9449, Val Acc = 64.95%\n",
      "Test Accuracy = 64.61%\n",
      "Model Size (state_dict): 42.73 MB\n",
      "Avg Inference Time per Batch: 0.1330 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.13296038703580873"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Train baseline ===\n",
    "print(\"=== Train baseline ===\")\n",
    "train(resnet18, train_loader, val_loader, epochs=1)\n",
    "evaluate(resnet18, test_loader)\n",
    "model_size_mb(resnet18)\n",
    "measure_inference_speed(resnet18, test_loader)\n",
    "\n",
    "# === Apply structured pruning, then fine-tune ===\n",
    "print(\"=== Apply structured pruning, then fine-tune ===\")\n",
    "resnet18_pruned_st = structured_prune_cnn(resnet18, amount=0.5)\n",
    "train(resnet18_pruned_st, train_loader, val_loader, epochs=1, lr=1e-5)\n",
    "evaluate(resnet18_pruned_st, test_loader)\n",
    "model_size_mb(resnet18_pruned_st)\n",
    "measure_inference_speed(resnet18_pruned_st, test_loader)\n",
    "\n",
    "# === Apply unstructured pruning, then fine-tune ===\n",
    "print(\"=== Apply unstructured pruning, then fine-tune ===\")\n",
    "resnet18_pruned_unst = unstructured_prune_cnn(resnet18, amount=0.5)\n",
    "train(resnet18_pruned_unst, train_loader, val_loader, epochs=1, lr=1e-5)\n",
    "evaluate(resnet18_pruned_unst, test_loader)\n",
    "model_size_mb(resnet18_pruned_unst)\n",
    "measure_inference_speed(resnet18_pruned_unst, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Apply quantization on baseline (no pruning) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smahadi/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 48.16%\n",
      "Model Size (state_dict): 10.79 MB\n",
      "Avg Inference Time per Batch: 0.1500 sec\n",
      "=== Apply quantization on pruned model ===\n",
      "Test Accuracy = 44.82%\n",
      "Model Size (state_dict): 10.79 MB\n",
      "Avg Inference Time per Batch: 0.1555 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15552488681489388"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Apply quantization on baseline (no pruning) ===\n",
    "print(\"=== Apply quantization on baseline (no pruning) ===\")\n",
    "resnet18_quant = quantize_model(resnet18, train_loader)\n",
    "evaluate(resnet18_quant, test_loader, quantization=True)\n",
    "model_size_mb(resnet18_quant)\n",
    "measure_inference_speed(resnet18_quant, test_loader, quantization=True)\n",
    "\n",
    "# === Apply quantization on pruned model ===\n",
    "print(\"=== Apply quantization on pruned model ===\")\n",
    "resnet18_quant_pr = quantize_trained_pruned_model(resnet18_pruned_st, train_loader)\n",
    "evaluate(resnet18_quant_pr, test_loader, quantization=True)\n",
    "model_size_mb(resnet18_quant_pr)\n",
    "measure_inference_speed(resnet18_quant_pr, test_loader, quantization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
