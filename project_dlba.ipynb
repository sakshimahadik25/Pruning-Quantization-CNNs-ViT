{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import timm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "FER_CLASSES = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "base_dir = 'fer2013/versions/1/train'\n",
    "\n",
    "# Auto-create missing class folders if not present\n",
    "for cls in FER_CLASSES:\n",
    "    cls_path = os.path.join(base_dir, cls)\n",
    "    if not os.path.exists(cls_path):\n",
    "        os.makedirs(cls_path)\n",
    "        print(f\"⚠️ Created empty folder: {cls_path}\")\n",
    "\n",
    "# Remove unsupported or hidden files\n",
    "for cls in FER_CLASSES:\n",
    "    cls_path = os.path.join(base_dir, cls)\n",
    "    if not os.path.exists(cls_path): continue\n",
    "    for f in os.listdir(cls_path):\n",
    "        full_path = os.path.join(cls_path, f)\n",
    "        if not f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp')):\n",
    "            print(f\"⛔ Skipping invalid file: {full_path}\")\n",
    "            os.remove(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry: 958 images\n",
      "disgust: 111 images\n",
      "fear: 1024 images\n",
      "happy: 1774 images\n",
      "sad: 1247 images\n",
      "surprise: 831 images\n",
      "neutral: 1233 images\n"
     ]
    }
   ],
   "source": [
    "for cls in FER_CLASSES:\n",
    "    cls_path = os.path.join('fer2013/versions/1/test', cls)\n",
    "    files = [f for f in os.listdir(cls_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'))]\n",
    "    print(f\"{cls}: {len(files)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/msambare/fer2013?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60.3M/60.3M [00:00<00:00, 86.8MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/smahadi/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"msambare/fer2013\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Required for ViTs/CNNs expecting 3 channels\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(root='fer2013/versions/1/train', transform=transform)\n",
    "test_dataset = ImageFolder(root='fer2013/versions/1/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Models\n",
    "vgg11 = timm.create_model(\"vgg11\", pretrained=True, num_classes=7).to(device)\n",
    "resnet18 = timm.create_model(\"resnet18\", pretrained=True, num_classes=7).to(device)\n",
    "\n",
    "# Vision Transformer\n",
    "vit = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=7).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, epochs=5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        correct = total = 0\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            correct += (out.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        acc = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}: Accuracy = {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            correct += (out.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Test Accuracy = {acc:.2f}%\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "def apply_pruning(model, amount=0.3):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model):\n",
    "    model.eval()\n",
    "    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "    model_fp32_prepared = torch.quantization.prepare(model, inplace=False)\n",
    "\n",
    "    # Run a few batches to calibrate\n",
    "    with torch.no_grad():\n",
    "        for x, _ in train_loader:\n",
    "            x = x.to(device)\n",
    "            model_fp32_prepared(x)\n",
    "            break\n",
    "\n",
    "    quantized_model = torch.quantization.convert(model_fp32_prepared, inplace=False)\n",
    "    return quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency(model, loader):\n",
    "    model.eval()\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)\n",
    "            _ = model(x)\n",
    "    end = time.time()\n",
    "    latency = (end - start) / len(loader)\n",
    "    print(f\"Avg. Inference Latency: {latency:.4f} seconds per batch\")\n",
    "    return latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_size_mb(model):\n",
    "    param_size = sum(p.numel() for p in model.parameters()) * 4 / (1024 ** 2)\n",
    "    print(f\"Model Size: {param_size:.2f} MB\")\n",
    "    return param_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training ResNet18 (Baseline) ---\n",
      "Epoch 1: Accuracy = 36.00%\n",
      "Epoch 2: Accuracy = 53.44%\n",
      "Epoch 3: Accuracy = 60.33%\n",
      "Epoch 4: Accuracy = 65.48%\n",
      "Epoch 5: Accuracy = 70.59%\n",
      "Test Accuracy = 61.27%\n",
      "Model Size: 42.65 MB\n",
      "Avg. Inference Latency: 0.1168 seconds per batch\n",
      "\n",
      "--- Pruning ResNet18 ---\n",
      "Epoch 1: Accuracy = 74.39%\n",
      "Epoch 2: Accuracy = 78.57%\n",
      "Epoch 3: Accuracy = 82.69%\n",
      "Epoch 4: Accuracy = 86.69%\n",
      "Epoch 5: Accuracy = 90.39%\n",
      "Test Accuracy = 62.87%\n",
      "Model Size: 42.65 MB\n",
      "Avg. Inference Latency: 0.1170 seconds per batch\n",
      "\n",
      "--- Quantizing ResNet18 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smahadi/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported qscheme: per_channel_affine",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Apply quantization\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Quantizing ResNet18 ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m resnet18_quant \u001b[38;5;241m=\u001b[39m \u001b[43mquantize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet18\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m evaluate(resnet18_quant, test_loader)\n\u001b[1;32m     20\u001b[0m model_size_mb(resnet18_quant)\n",
      "Cell \u001b[0;32mIn[24], line 13\u001b[0m, in \u001b[0;36mquantize_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     10\u001b[0m         model_fp32_prepared(x)\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_fp32_prepared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m quantized_model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/quantization/quantize.py:657\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[1;32m    656\u001b[0m     module \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(module)\n\u001b[0;32m--> 657\u001b[0m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_qconfig:\n\u001b[1;32m    666\u001b[0m     _remove_qconfig(module)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/quantization/quantize.py:722\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule)\n\u001b[1;32m    712\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping\n\u001b[1;32m    713\u001b[0m     ):\n\u001b[1;32m    714\u001b[0m         _convert(\n\u001b[1;32m    715\u001b[0m             mod,\n\u001b[1;32m    716\u001b[0m             mapping,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    720\u001b[0m             use_precomputed_fake_quant\u001b[38;5;241m=\u001b[39muse_precomputed_fake_quant,\n\u001b[1;32m    721\u001b[0m         )\n\u001b[0;32m--> 722\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m \u001b[43mswap_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_module_class_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    727\u001b[0m     module\u001b[38;5;241m.\u001b[39m_modules[key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/quantization/quantize.py:764\u001b[0m, in \u001b[0;36mswap_module\u001b[0;34m(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)\u001b[0m\n\u001b[1;32m    762\u001b[0m sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(qmod\u001b[38;5;241m.\u001b[39mfrom_float)\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_precomputed_fake_quant\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mparameters:\n\u001b[0;32m--> 764\u001b[0m     new_mod \u001b[38;5;241m=\u001b[39m \u001b[43mqmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_float\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_precomputed_fake_quant\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    768\u001b[0m     new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(mod)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/conv.py:606\u001b[0m, in \u001b[0;36mConv2d.from_float\u001b[0;34m(cls, mod, use_precomputed_fake_quant)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_float\u001b[39m(\u001b[38;5;28mcls\u001b[39m, mod, use_precomputed_fake_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    600\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a quantized module from a float module or qparams_dict.\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;124;03m        mod (Module): a float module, either produced by torch.ao.quantization\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;124;03m          utilities or provided by the user\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ConvNd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_float\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_precomputed_fake_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_precomputed_fake_quant\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/conv.py:322\u001b[0m, in \u001b[0;36m_ConvNd.from_float\u001b[0;34m(cls, mod, use_precomputed_fake_quant)\u001b[0m\n\u001b[1;32m    320\u001b[0m         mod \u001b[38;5;241m=\u001b[39m mod[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    321\u001b[0m     weight_post_process \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39mqconfig\u001b[38;5;241m.\u001b[39mweight()\n\u001b[0;32m--> 322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_qconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_post_process\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_post_process\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/conv.py:266\u001b[0m, in \u001b[0;36m_ConvNd.get_qconv\u001b[0;34m(cls, mod, activation_post_process, weight_post_process)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# the __init__ call used is the one from derived classes and not the one from _ConvNd\u001b[39;00m\n\u001b[1;32m    255\u001b[0m qconv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    256\u001b[0m     mod\u001b[38;5;241m.\u001b[39min_channels,\n\u001b[1;32m    257\u001b[0m     mod\u001b[38;5;241m.\u001b[39mout_channels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m     mod\u001b[38;5;241m.\u001b[39mpadding_mode,\n\u001b[1;32m    265\u001b[0m )\n\u001b[0;32m--> 266\u001b[0m \u001b[43mqconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_weight_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    268\u001b[0m     activation_post_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m activation_post_process\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat\n\u001b[1;32m    270\u001b[0m ):\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m qconv  \u001b[38;5;66;03m# dynamic quantization doesn't need scale/zero_point\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/conv.py:567\u001b[0m, in \u001b[0;36mConv2d.set_weight_bias\u001b[0;34m(self, w, b)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mset_weight_bias\u001b[39m(\u001b[38;5;28mself\u001b[39m, w: torch\u001b[38;5;241m.\u001b[39mTensor, b: Optional[torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 567\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d_prepack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m            \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    571\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mconv2d_prepack(\n\u001b[1;32m    572\u001b[0m             w, b, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    573\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_ops.py:1123\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[0;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unsupported qscheme: per_channel_affine"
     ]
    }
   ],
   "source": [
    "# Train baseline ResNet18\n",
    "print(\"\\n--- Training ResNet18 (Baseline) ---\")\n",
    "train(resnet18, train_loader, epochs=5)\n",
    "evaluate(resnet18, test_loader)\n",
    "model_size_mb(resnet18)\n",
    "measure_latency(resnet18, test_loader)\n",
    "\n",
    "# Apply pruning\n",
    "print(\"\\n--- Pruning ResNet18 ---\")\n",
    "resnet18_pruned = apply_pruning(resnet18, amount=0.3)\n",
    "train(resnet18_pruned, train_loader, epochs=5)\n",
    "evaluate(resnet18_pruned, test_loader)\n",
    "model_size_mb(resnet18_pruned)\n",
    "measure_latency(resnet18_pruned, test_loader)\n",
    "\n",
    "# Apply quantization\n",
    "print(\"\\n--- Quantizing ResNet18 ---\")\n",
    "resnet18_quant = quantize_model(resnet18)\n",
    "evaluate(resnet18_quant, test_loader)\n",
    "model_size_mb(resnet18_quant)\n",
    "measure_latency(resnet18_quant, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline ResNet18\n",
    "print(\"\\n--- Training VGG11 (Baseline) ---\")\n",
    "train(vgg11, train_loader, epochs=5)\n",
    "evaluate(vgg11, test_loader)\n",
    "model_size_mb(vgg11)\n",
    "measure_latency(vgg11, test_loader)\n",
    "\n",
    "# Apply pruning\n",
    "print(\"\\n--- Pruning VGG11 ---\")\n",
    "vgg11_pruned = apply_pruning(vgg11, amount=0.3)\n",
    "evaluate( vgg11_pruned, test_loader)\n",
    "model_size_mb(vgg11_pruned)\n",
    "measure_latency(vgg11_pruned, test_loader)\n",
    "\n",
    "# Apply quantization\n",
    "print(\"\\n--- Quantizing VGG11 ---\")\n",
    "vgg11_quant = quantize_model(vgg11)\n",
    "evaluate(vgg11_quant, test_loader)\n",
    "model_size_mb(vgg11_quant)\n",
    "measure_latency(vgg11_quant, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9179"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del resnet18\n",
    "del resnet18_pruned\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
