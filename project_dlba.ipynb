{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.quantization\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import timm\n",
    "import copy\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"msambare/fer2013\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "dataset = ImageFolder('fer2013/versions/1/train', transform=transform)\n",
    "val_size = int(0.2 * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "test_dataset = ImageFolder('fer2013/versions/1/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Check distribution\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.quantization import resnet18 as resnet18_model\n",
    "\n",
    "resnet18 = resnet18_model(pretrained=True, quantize=False)\n",
    "resnet18.fc = nn.Linear(512, 7)\n",
    "resnet18.eval()\n",
    "resnet18.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstructured Pruning for CNN\n",
    "def unstructured_prune_cnn(model, amount=0.3):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
    "            prune.remove(module, \"weight\")\n",
    "    return model\n",
    "\n",
    "# Structured Pruning for CNN\n",
    "def structured_prune_cnn(model, amount=0.3):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            prune.ln_structured(module, name=\"weight\", amount=amount, n=2, dim=0)\n",
    "            prune.remove(module, \"weight\")  # REQUIRED!\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstructured Pruning for ViT\n",
    "def unstructured_prune_vit(model, amount=0.3):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
    "            prune.remove(module, \"weight\")\n",
    "    return model\n",
    "\n",
    "# Structured Attention Head Pruning for ViT\n",
    "def prune_vit_attention_heads(model, heads_to_prune=2):\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'qkv') and hasattr(module, 'num_heads'):\n",
    "            heads_dim = module.qkv.weight.shape[0] // 3\n",
    "            head_size = heads_dim // module.num_heads\n",
    "            qkv_weights = module.qkv.weight.data.view(3, module.num_heads, head_size, -1)\n",
    "            norms = qkv_weights.norm(dim=(2, 3))\n",
    "            importance = norms.sum(dim=0)\n",
    "            prune_indices = torch.topk(importance, heads_to_prune, largest=False).indices\n",
    "            for i in prune_indices:\n",
    "                qkv_weights[:, i, :, :] = 0\n",
    "            module.qkv.weight.data = qkv_weights.view(-1, module.qkv.weight.shape[1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model, calibration_loader):\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    model.fuse_model()\n",
    "\n",
    "    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "    torch.quantization.prepare(model, inplace=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in calibration_loader:\n",
    "            images = images.to(torch.float32).cpu()\n",
    "            model(images)\n",
    "\n",
    "    torch.quantization.convert(model, inplace=True)\n",
    "    return model\n",
    "\n",
    "def quantize_vgg_dynamic(model):\n",
    "    return torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "\n",
    "def quantize_vit_dynamic(model):\n",
    "    return torch.quantization.quantize_dynamic(model.to(\"cpu\"), {nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs=10, lr=1e-4):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss += loss.item()\n",
    "                correct += (outputs.argmax(1) == y).sum().item()\n",
    "                total += y.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100 * correct / total\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, quantization=False):\n",
    "    model.eval()\n",
    "\n",
    "    if quantization:\n",
    "        model = model.to(\"cpu\")  # Quantized models must be on CPU\n",
    "    else:\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            if quantization:\n",
    "                x, y = x.to(\"cpu\"), y.to(\"cpu\")\n",
    "            else:\n",
    "                x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "\n",
    "            outputs = model(x)\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy = {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_speed(model, test_loader, quantization=False):\n",
    "    model.eval()\n",
    "    \n",
    "    if quantization:\n",
    "        device = \"cpu\"\n",
    "        \n",
    "    else:\n",
    "        device = torch.device(\"cuda\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for x, _ in test_loader:\n",
    "            x = x.to(device)\n",
    "            _ = model(x)\n",
    "    end = time.time()\n",
    "    latency = (end - start) / len(test_loader)\n",
    "    print(f\"Avg Inference Time per Batch: {latency:.4f} sec\")\n",
    "    return latency\n",
    "\n",
    "def model_size_mb(model, use_state_dict=True):\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "        if use_state_dict:\n",
    "            torch.save(model.state_dict(), f.name)\n",
    "        else:\n",
    "            torch.save(model, f.name)\n",
    "        size_mb = os.path.getsize(f.name) / (1024 * 1024)\n",
    "    print(f\"Model Size ({'state_dict' if use_state_dict else 'full model'}): {size_mb:.2f} MB\")\n",
    "    return size_mb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_pruned_model(model, dataloader):\n",
    "    import torch\n",
    "    import torch.quantization\n",
    "\n",
    "    # Make sure model is in eval mode and on CPU\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "\n",
    "    # Fuse top-level modules\n",
    "    fused_model = torch.quantization.fuse_modules(\n",
    "        model,\n",
    "        [[\"conv1\", \"bn1\", \"relu\"]],\n",
    "        inplace=False\n",
    "    )\n",
    "\n",
    "    # Fuse residual blocks\n",
    "    for name, module in fused_model.named_children():\n",
    "        if \"layer\" in name:\n",
    "            for block in module:\n",
    "                torch.quantization.fuse_modules(\n",
    "                    block,\n",
    "                    [[\"conv1\", \"bn1\", \"relu\"], [\"conv2\", \"bn2\"]],\n",
    "                    inplace=True\n",
    "                )\n",
    "\n",
    "    # Set qconfig and prepare for calibration\n",
    "    fused_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "    torch.quantization.prepare(fused_model, inplace=True)\n",
    "\n",
    "    # Calibrate\n",
    "    with torch.no_grad():\n",
    "        for i, (images, _) in enumerate(dataloader):\n",
    "            if i >= 10:\n",
    "                break\n",
    "            images = images.to(torch.float32).cpu()\n",
    "            fused_model(images)\n",
    "\n",
    "    # Convert to quantized model\n",
    "    quantized_model = torch.quantization.convert(fused_model, inplace=False)\n",
    "    return quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Apply quantization on baseline (no pruning) ===\n",
    "print(\"=== Apply quantization on baseline (no pruning) ===\")\n",
    "resnet18_quant = quantize_model(resnet18, train_loader)\n",
    "evaluate(resnet18_quant, test_loader, quantization=True)\n",
    "model_size_mb(resnet18_quant)\n",
    "measure_inference_speed(resnet18_quant, test_loader, quantization=True)\n",
    "\n",
    "# === Apply quantization on pruned model ===\n",
    "print(\"=== Apply quantization on pruned model ===\")\n",
    "resnet18_quant_pr = quantize_pruned_model(resnet18_pruned_st, train_loader)\n",
    "evaluate(resnet18_quant_pr, test_loader, quantization=True)\n",
    "model_size_mb(resnet18_quant_pr)\n",
    "measure_inference_speed(resnet18_quant_pr, test_loader, quantization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def quantize_trained_pruned_model(model, dataloader, num_calibration_batches=10):\n",
    "    \"\"\"\n",
    "    Applies static quantization to a pruned + trained ResNet18 model.\n",
    "    - Fuses modules\n",
    "    - Prepares with qconfig\n",
    "    - Calibrates using dataloader\n",
    "    - Converts to quantized model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "\n",
    "    # Fuse top-level layers\n",
    "    fused_model = torch.quantization.fuse_modules(\n",
    "        model,\n",
    "        [[\"conv1\", \"bn1\", \"relu\"]],\n",
    "        inplace=False\n",
    "    )\n",
    "\n",
    "    # Fuse residual blocks\n",
    "    for name, module in fused_model.named_children():\n",
    "        if \"layer\" in name:\n",
    "            for block in module:\n",
    "                torch.quantization.fuse_modules(\n",
    "                    block,\n",
    "                    [[\"conv1\", \"bn1\", \"relu\"], [\"conv2\", \"bn2\"]],\n",
    "                    inplace=True\n",
    "                )\n",
    "\n",
    "    # Attach quantization config\n",
    "    fused_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "    # Prepare for calibration\n",
    "    torch.quantization.prepare(fused_model, inplace=True)\n",
    "\n",
    "    # Calibrate on a few batches\n",
    "    fused_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, _) in enumerate(dataloader):\n",
    "            if i >= num_calibration_batches:\n",
    "                break\n",
    "            images = images.to(torch.float32).cpu()\n",
    "            fused_model(images)\n",
    "\n",
    "    # Convert to quantized model\n",
    "    quantized_model = torch.quantization.convert(fused_model, inplace=False)\n",
    "    return quantized_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Train baseline ===\n",
    "print(\"=== Train baseline ===\")\n",
    "train(resnet18, train_loader, val_loader, epochs=1)\n",
    "evaluate(resnet18, test_loader)\n",
    "model_size_mb(resnet18)\n",
    "measure_inference_speed(resnet18, test_loader)\n",
    "\n",
    "# === Apply structured pruning, then fine-tune ===\n",
    "print(\"=== Apply structured pruning, then fine-tune ===\")\n",
    "resnet18_pruned_st = structured_prune_cnn(resnet18, amount=0.5)\n",
    "train(resnet18_pruned_st, train_loader, val_loader, epochs=1, lr=1e-5)\n",
    "evaluate(resnet18_pruned_st, test_loader)\n",
    "model_size_mb(resnet18_pruned_st)\n",
    "measure_inference_speed(resnet18_pruned_st, test_loader)\n",
    "\n",
    "# === Apply unstructured pruning, then fine-tune ===\n",
    "print(\"=== Apply unstructured pruning, then fine-tune ===\")\n",
    "resnet18_pruned_unst = unstructured_prune_cnn(resnet18, amount=0.5)\n",
    "train(resnet18_pruned_unst, train_loader, val_loader, epochs=1, lr=1e-5)\n",
    "evaluate(resnet18_pruned_unst, test_loader)\n",
    "model_size_mb(resnet18_pruned_unst)\n",
    "measure_inference_speed(resnet18_pruned_unst, test_loader)\n",
    "\n",
    "# === Apply quantization on baseline (no pruning) ===\n",
    "print(\"=== Apply quantization on baseline (no pruning) ===\")\n",
    "resnet18_quant = quantize_model(resnet18, train_loader)\n",
    "evaluate(resnet18_quant, test_loader, quantization=True)\n",
    "model_size_mb(resnet18_quant)\n",
    "measure_inference_speed(resnet18_quant, test_loader, quantization=True)\n",
    "\n",
    "# === Apply quantization on pruned model ===\n",
    "print(\"=== Apply quantization on pruned model ===\")\n",
    "resnet18_quant_pr = quantize_trained_pruned_model(resnet18_pruned_st, train_loader)\n",
    "evaluate(resnet18_quant_pr, test_loader, quantization=True)\n",
    "model_size_mb(resnet18_quant_pr)\n",
    "measure_inference_speed(resnet18_quant_pr, test_loader, quantization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Apply quantization on pruned model ===\n",
    "print(\"=== Apply quantization on pruned model ===\")\n",
    "resnet18_quant_pr = quantize_pruned_model(resnet18_pruned_st, train_loader)\n",
    "evaluate(resnet18_quant_pr, test_loader, quantization=True)\n",
    "model_size_mb(resnet18_quant_pr)\n",
    "measure_inference_speed(resnet18_quant_pr, test_loader, quantization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Apply quantization on baseline (no pruning) ===\")\n",
    "resnet18_quant = quantize_model(resnet18, train_loader)\n",
    "#evaluate(resnet18_quant, test_loader, quantization=True)\n",
    "model_size_mb(resnet18_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Quantizing trained pruned model ===\")\n",
    "resnet18_quant_pr = quantize_trained_pruned_model(resnet18_pruned_st, train_loader)\n",
    "#evaluate(resnet18_quant_pr, test_loader, quantization=True)\n",
    "model_size_mb(resnet18_quant_pr, use_state_dict=False)\n",
    "#measure_inference_speed(resnet18_quant_pr, test_loader, quantization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
