{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smahadi/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.quantization\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import timm\n",
    "import copy\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/smahadi/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"msambare/fer2013\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 22968\n",
      "Validation samples: 5741\n",
      "Test samples: 7178\n"
     ]
    }
   ],
   "source": [
    "# Dataset loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "dataset = ImageFolder('fer2013/versions/1/train', transform=transform)\n",
    "val_size = int(0.2 * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "test_dataset = ImageFolder('fer2013/versions/1/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Check distribution\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smahadi/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/smahadi/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizableResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): QuantizableBasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): QuantizableBasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): QuantizableBasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): QuantizableBasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): QuantizableBasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): QuantizableBasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): QuantizableBasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): QuantizableBasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (add_relu): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=7, bias=True)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.quantization import resnet18 as resnet18_model\n",
    "\n",
    "resnet18 = resnet18_model(pretrained=True, quantize=False)\n",
    "resnet18.fc = nn.Linear(512, 7)\n",
    "resnet18.eval()\n",
    "resnet18.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstructured Pruning for CNN\n",
    "def unstructured_prune_cnn(model, amount=0.3):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
    "            prune.remove(module, \"weight\")\n",
    "    return model\n",
    "\n",
    "# Structured Pruning for CNN\n",
    "def structured_prune_cnn(model, amount=0.3):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            prune.ln_structured(module, name=\"weight\", amount=amount, n=2, dim=0)\n",
    "            prune.remove(module, \"weight\")  # REQUIRED!\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstructured Pruning for ViT\n",
    "def unstructured_prune_vit(model, amount=0.3):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
    "            prune.remove(module, \"weight\")\n",
    "    return model\n",
    "\n",
    "# Structured Attention Head Pruning for ViT\n",
    "def prune_vit_attention_heads(model, heads_to_prune=2):\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'qkv') and hasattr(module, 'num_heads'):\n",
    "            heads_dim = module.qkv.weight.shape[0] // 3\n",
    "            head_size = heads_dim // module.num_heads\n",
    "            qkv_weights = module.qkv.weight.data.view(3, module.num_heads, head_size, -1)\n",
    "            norms = qkv_weights.norm(dim=(2, 3))\n",
    "            importance = norms.sum(dim=0)\n",
    "            prune_indices = torch.topk(importance, heads_to_prune, largest=False).indices\n",
    "            for i in prune_indices:\n",
    "                qkv_weights[:, i, :, :] = 0\n",
    "            module.qkv.weight.data = qkv_weights.view(-1, module.qkv.weight.shape[1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model, calibration_loader):\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    model.fuse_model()\n",
    "\n",
    "    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "    torch.quantization.prepare(model, inplace=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in calibration_loader:\n",
    "            images = images.to(torch.float32).cpu()\n",
    "            model(images)\n",
    "\n",
    "    torch.quantization.convert(model, inplace=True)\n",
    "    return model\n",
    "\n",
    "def quantize_vgg_dynamic(model):\n",
    "    return torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "\n",
    "def quantize_vit_dynamic(model):\n",
    "    return torch.quantization.quantize_dynamic(model.to(\"cpu\"), {nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs=10, lr=1e-4):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss += loss.item()\n",
    "                correct += (outputs.argmax(1) == y).sum().item()\n",
    "                total += y.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100 * correct / total\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, quantization=False):\n",
    "    model.eval()\n",
    "\n",
    "    if quantization:\n",
    "        model = model.to(\"cpu\")  # Quantized models must be on CPU\n",
    "    else:\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            if quantization:\n",
    "                x, y = x.to(\"cpu\"), y.to(\"cpu\")\n",
    "            else:\n",
    "                x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "\n",
    "            outputs = model(x)\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy = {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_speed(model, test_loader, quantization=False):\n",
    "    model.eval()\n",
    "    \n",
    "    if quantization:\n",
    "        device = \"cpu\"\n",
    "        \n",
    "    else:\n",
    "        device = torch.device(\"cuda\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for x, _ in test_loader:\n",
    "            x = x.to(device)\n",
    "            _ = model(x)\n",
    "    end = time.time()\n",
    "    latency = (end - start) / len(test_loader)\n",
    "    print(f\"Avg Inference Time per Batch: {latency:.4f} sec\")\n",
    "    return latency\n",
    "\n",
    "def model_size_mb(model, use_state_dict=True):\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "        if use_state_dict:\n",
    "            torch.save(model.state_dict(), f.name)\n",
    "        else:\n",
    "            torch.save(model, f.name)\n",
    "        size_mb = os.path.getsize(f.name) / (1024 * 1024)\n",
    "    print(f\"Model Size ({'state_dict' if use_state_dict else 'full model'}): {size_mb:.2f} MB\")\n",
    "    return size_mb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_pruned_model(model, dataloader):\n",
    "    import torch\n",
    "    import torch.quantization\n",
    "\n",
    "    # Make sure model is in eval mode and on CPU\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "\n",
    "    # Fuse top-level modules\n",
    "    fused_model = torch.quantization.fuse_modules(\n",
    "        model,\n",
    "        [[\"conv1\", \"bn1\", \"relu\"]],\n",
    "        inplace=False\n",
    "    )\n",
    "\n",
    "    # Fuse residual blocks\n",
    "    for name, module in fused_model.named_children():\n",
    "        if \"layer\" in name:\n",
    "            for block in module:\n",
    "                torch.quantization.fuse_modules(\n",
    "                    block,\n",
    "                    [[\"conv1\", \"bn1\", \"relu\"], [\"conv2\", \"bn2\"]],\n",
    "                    inplace=True\n",
    "                )\n",
    "\n",
    "    # Set qconfig and prepare for calibration\n",
    "    fused_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "    torch.quantization.prepare(fused_model, inplace=True)\n",
    "\n",
    "    # Calibrate\n",
    "    with torch.no_grad():\n",
    "        for i, (images, _) in enumerate(dataloader):\n",
    "            if i >= 10:\n",
    "                break\n",
    "            images = images.to(torch.float32).cpu()\n",
    "            fused_model(images)\n",
    "\n",
    "    # Convert to quantized model\n",
    "    quantized_model = torch.quantization.convert(fused_model, inplace=False)\n",
    "    return quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Apply quantization on baseline (no pruning) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smahadi/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 59.86%\n",
      "Model Size: 10.79 MB\n",
      "Avg Inference Time per Batch: 0.1627 sec\n",
      "=== Apply quantization on pruned model ===\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "did not find fuser method for: (<class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>, <class 'torch.nn.modules.linear.Identity'>, <class 'torch.nn.modules.linear.Identity'>) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# === Apply quantization on pruned model ===\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Apply quantization on pruned model ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m resnet18_quant_pr \u001b[38;5;241m=\u001b[39m \u001b[43mquantize_pruned_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet18_pruned_st\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m evaluate(resnet18_quant_pr, test_loader, quantization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m model_size_mb(resnet18_quant_pr)\n",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m, in \u001b[0;36mquantize_pruned_model\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mquantize_pruned_model\u001b[39m(model, dataloader):\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m----> 3\u001b[0m     fused_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfuse_modules\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconv1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbn1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m fused_model\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/quantization/fuse_modules.py:191\u001b[0m, in \u001b[0;36mfuse_modules\u001b[0;34m(model, modules_to_fuse, inplace, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfuse_modules\u001b[39m(\n\u001b[1;32m    131\u001b[0m     model,\n\u001b[1;32m    132\u001b[0m     modules_to_fuse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m     fuse_custom_config_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    136\u001b[0m ):\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Fuse a list of modules into a single module.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    Fuses only the following sequence of modules:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fuse_modules\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodules_to_fuse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_qat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuser_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuser_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuse_custom_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuse_custom_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/quantization/fuse_modules.py:124\u001b[0m, in \u001b[0;36m_fuse_modules\u001b[0;34m(model, modules_to_fuse, is_qat, inplace, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# Handle case of modules_to_fuse being a list of lists\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module_list \u001b[38;5;129;01min\u001b[39;00m modules_to_fuse:\n\u001b[0;32m--> 124\u001b[0m         \u001b[43m_fuse_modules_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_qat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuser_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse_custom_config_dict\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/quantization/fuse_modules.py:98\u001b[0m, in \u001b[0;36m_fuse_modules_helper\u001b[0;34m(model, modules_to_fuse, is_qat, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[1;32m     95\u001b[0m mod_list \u001b[38;5;241m=\u001b[39m [_get_module(model, item) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m modules_to_fuse]\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Fuse list of modules\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m new_mod_list \u001b[38;5;241m=\u001b[39m \u001b[43mfuser_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_qat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_fuser_method_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Replace original module list with fused module list\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules_to_fuse):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/quantization/fuse_modules.py:59\u001b[0m, in \u001b[0;36mfuse_known_modules\u001b[0;34m(mod_list, is_qat, additional_fuser_method_mapping)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of known fuse modules.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03mReturns a list of modules that fuses the operations specified\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mthe fused operation. The rest of the elements are set to nn.Identity()\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(type_before_parametrizations(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m mod_list)\n\u001b[0;32m---> 59\u001b[0m fuser_method \u001b[38;5;241m=\u001b[39m \u001b[43mget_fuser_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_fuser_method_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fuser_method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot fuse modules: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/quantization/fuser_method_mappings.py:226\u001b[0m, in \u001b[0;36mget_fuser_method\u001b[0;34m(op_list, additional_fuser_method_mapping)\u001b[0m\n\u001b[1;32m    222\u001b[0m all_mappings \u001b[38;5;241m=\u001b[39m get_combined_dict(\n\u001b[1;32m    223\u001b[0m     _DEFAULT_OP_LIST_TO_FUSER_METHOD, additional_fuser_method_mapping\n\u001b[1;32m    224\u001b[0m )\n\u001b[1;32m    225\u001b[0m fuser_method \u001b[38;5;241m=\u001b[39m all_mappings\u001b[38;5;241m.\u001b[39mget(op_list, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m fuser_method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdid not find fuser method for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fuser_method\n",
      "\u001b[0;31mAssertionError\u001b[0m: did not find fuser method for: (<class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>, <class 'torch.nn.modules.linear.Identity'>, <class 'torch.nn.modules.linear.Identity'>) "
     ]
    }
   ],
   "source": [
    "# === Apply quantization on baseline (no pruning) ===\n",
    "print(\"=== Apply quantization on baseline (no pruning) ===\")\n",
    "resnet18_quant = quantize_model(resnet18, train_loader)\n",
    "evaluate(resnet18_quant, test_loader, quantization=True)\n",
    "model_size_mb(resnet18_quant)\n",
    "measure_inference_speed(resnet18_quant, test_loader, quantization=True)\n",
    "\n",
    "# === Apply quantization on pruned model ===\n",
    "print(\"=== Apply quantization on pruned model ===\")\n",
    "resnet18_quant_pr = quantize_pruned_model(resnet18_pruned_st, train_loader)\n",
    "evaluate(resnet18_quant_pr, test_loader, quantization=True)\n",
    "model_size_mb(resnet18_quant_pr)\n",
    "measure_inference_speed(resnet18_quant_pr, test_loader, quantization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def quantize_trained_pruned_model(model, dataloader, num_calibration_batches=10):\n",
    "    \"\"\"\n",
    "    Applies static quantization to a pruned + trained ResNet18 model.\n",
    "    - Fuses modules\n",
    "    - Prepares with qconfig\n",
    "    - Calibrates using dataloader\n",
    "    - Converts to quantized model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "\n",
    "    # Fuse top-level layers\n",
    "    fused_model = torch.quantization.fuse_modules(\n",
    "        model,\n",
    "        [[\"conv1\", \"bn1\", \"relu\"]],\n",
    "        inplace=False\n",
    "    )\n",
    "\n",
    "    # Fuse residual blocks\n",
    "    for name, module in fused_model.named_children():\n",
    "        if \"layer\" in name:\n",
    "            for block in module:\n",
    "                torch.quantization.fuse_modules(\n",
    "                    block,\n",
    "                    [[\"conv1\", \"bn1\", \"relu\"], [\"conv2\", \"bn2\"]],\n",
    "                    inplace=True\n",
    "                )\n",
    "\n",
    "    # Attach quantization config\n",
    "    fused_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "    # Prepare for calibration\n",
    "    torch.quantization.prepare(fused_model, inplace=True)\n",
    "\n",
    "    # Calibrate on a few batches\n",
    "    fused_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, _) in enumerate(dataloader):\n",
    "            if i >= num_calibration_batches:\n",
    "                break\n",
    "            images = images.to(torch.float32).cpu()\n",
    "            fused_model(images)\n",
    "\n",
    "    # Convert to quantized model\n",
    "    quantized_model = torch.quantization.convert(fused_model, inplace=False)\n",
    "    return quantized_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Apply structured pruning, then fine-tune ===\n",
      "Epoch 1: Train Loss = 1.4249, Val Loss = 1.2460, Val Acc = 52.95%\n",
      "Model Size (state_dict): 42.73 MB\n",
      "=== Apply quantization on pruned model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smahadi/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size (state_dict): 10.80 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.803594589233398"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Train baseline ===\n",
    "print(\"=== Train baseline ===\")\n",
    "train(resnet18, train_loader, val_loader, epochs=1)\n",
    "evaluate(resnet18, test_loader)\n",
    "model_size_mb(resnet18)\n",
    "measure_inference_speed(resnet18, test_loader)\n",
    "\n",
    "# === Apply structured pruning, then fine-tune ===\n",
    "print(\"=== Apply structured pruning, then fine-tune ===\")\n",
    "resnet18_pruned_st = structured_prune_cnn(resnet18, amount=0.5)\n",
    "train(resnet18_pruned_st, train_loader, val_loader, epochs=1, lr=1e-5)\n",
    "evaluate(resnet18_pruned_st, test_loader)\n",
    "model_size_mb(resnet18_pruned_st)\n",
    "measure_inference_speed(resnet18_pruned_st, test_loader)\n",
    "\n",
    "# === Apply unstructured pruning, then fine-tune ===\n",
    "print(\"=== Apply unstructured pruning, then fine-tune ===\")\n",
    "resnet18_pruned_unst = unstructured_prune_cnn(resnet18, amount=0.5)\n",
    "train(resnet18_pruned_unst, train_loader, val_loader, epochs=1, lr=1e-5)\n",
    "evaluate(resnet18_pruned_unst, test_loader)\n",
    "model_size_mb(resnet18_pruned_unst)\n",
    "measure_inference_speed(resnet18_pruned_unst, test_loader)\n",
    "\n",
    "# === Apply quantization on baseline (no pruning) ===\n",
    "print(\"=== Apply quantization on baseline (no pruning) ===\")\n",
    "resnet18_quant = quantize_model(resnet18, train_loader)\n",
    "evaluate(resnet18_quant, test_loader, quantization=True)\n",
    "model_size_mb(resnet18_quant)\n",
    "measure_inference_speed(resnet18_quant, test_loader, quantization=True)\n",
    "\n",
    "# === Apply quantization on pruned model ===\n",
    "print(\"=== Apply quantization on pruned model ===\")\n",
    "resnet18_quant_pr = quantize_trained_pruned_model(resnet18_pruned_st, train_loader)\n",
    "evaluate(resnet18_quant_pr, test_loader, quantization=True)\n",
    "model_size_mb(resnet18_quant_pr)\n",
    "measure_inference_speed(resnet18_quant_pr, test_loader, quantization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Apply quantization on pruned model ===\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "did not find fuser method for: (<class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>, <class 'torch.nn.modules.linear.Identity'>, <class 'torch.nn.modules.linear.Identity'>) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# === Apply quantization on pruned model ===\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Apply quantization on pruned model ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m resnet18_quant_pr \u001b[38;5;241m=\u001b[39m \u001b[43mquantize_pruned_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet18_pruned_st\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m evaluate(resnet18_quant_pr, test_loader, quantization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m model_size_mb(resnet18_quant_pr)\n",
      "Cell \u001b[0;32mIn[36], line 10\u001b[0m, in \u001b[0;36mquantize_pruned_model\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Fuse top-level modules\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m fused_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfuse_modules\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconv1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbn1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Fuse residual blocks\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m fused_model\u001b[38;5;241m.\u001b[39mnamed_children():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/quantization/fuse_modules.py:191\u001b[0m, in \u001b[0;36mfuse_modules\u001b[0;34m(model, modules_to_fuse, inplace, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfuse_modules\u001b[39m(\n\u001b[1;32m    131\u001b[0m     model,\n\u001b[1;32m    132\u001b[0m     modules_to_fuse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m     fuse_custom_config_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    136\u001b[0m ):\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Fuse a list of modules into a single module.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    Fuses only the following sequence of modules:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fuse_modules\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodules_to_fuse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_qat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuser_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuser_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuse_custom_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuse_custom_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/quantization/fuse_modules.py:124\u001b[0m, in \u001b[0;36m_fuse_modules\u001b[0;34m(model, modules_to_fuse, is_qat, inplace, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# Handle case of modules_to_fuse being a list of lists\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module_list \u001b[38;5;129;01min\u001b[39;00m modules_to_fuse:\n\u001b[0;32m--> 124\u001b[0m         \u001b[43m_fuse_modules_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_qat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuser_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuse_custom_config_dict\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/quantization/fuse_modules.py:98\u001b[0m, in \u001b[0;36m_fuse_modules_helper\u001b[0;34m(model, modules_to_fuse, is_qat, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[1;32m     95\u001b[0m mod_list \u001b[38;5;241m=\u001b[39m [_get_module(model, item) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m modules_to_fuse]\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Fuse list of modules\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m new_mod_list \u001b[38;5;241m=\u001b[39m \u001b[43mfuser_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_qat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_fuser_method_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Replace original module list with fused module list\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules_to_fuse):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/quantization/fuse_modules.py:59\u001b[0m, in \u001b[0;36mfuse_known_modules\u001b[0;34m(mod_list, is_qat, additional_fuser_method_mapping)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of known fuse modules.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03mReturns a list of modules that fuses the operations specified\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mthe fused operation. The rest of the elements are set to nn.Identity()\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(type_before_parametrizations(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m mod_list)\n\u001b[0;32m---> 59\u001b[0m fuser_method \u001b[38;5;241m=\u001b[39m \u001b[43mget_fuser_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_fuser_method_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fuser_method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot fuse modules: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/ao/quantization/fuser_method_mappings.py:226\u001b[0m, in \u001b[0;36mget_fuser_method\u001b[0;34m(op_list, additional_fuser_method_mapping)\u001b[0m\n\u001b[1;32m    222\u001b[0m all_mappings \u001b[38;5;241m=\u001b[39m get_combined_dict(\n\u001b[1;32m    223\u001b[0m     _DEFAULT_OP_LIST_TO_FUSER_METHOD, additional_fuser_method_mapping\n\u001b[1;32m    224\u001b[0m )\n\u001b[1;32m    225\u001b[0m fuser_method \u001b[38;5;241m=\u001b[39m all_mappings\u001b[38;5;241m.\u001b[39mget(op_list, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m fuser_method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdid not find fuser method for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fuser_method\n",
      "\u001b[0;31mAssertionError\u001b[0m: did not find fuser method for: (<class 'torch.ao.nn.intrinsic.quantized.modules.conv_relu.ConvReLU2d'>, <class 'torch.nn.modules.linear.Identity'>, <class 'torch.nn.modules.linear.Identity'>) "
     ]
    }
   ],
   "source": [
    "# === Apply quantization on pruned model ===\n",
    "print(\"=== Apply quantization on pruned model ===\")\n",
    "resnet18_quant_pr = quantize_pruned_model(resnet18_pruned_st, train_loader)\n",
    "evaluate(resnet18_quant_pr, test_loader, quantization=True)\n",
    "model_size_mb(resnet18_quant_pr)\n",
    "measure_inference_speed(resnet18_quant_pr, test_loader, quantization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Apply quantization on baseline (no pruning) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smahadi/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 10.79 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.787996292114258"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=== Apply quantization on baseline (no pruning) ===\")\n",
    "resnet18_quant = quantize_model(resnet18, train_loader)\n",
    "#evaluate(resnet18_quant, test_loader, quantization=True)\n",
    "model_size_mb(resnet18_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Quantizing trained pruned model ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'resnet18_pruned_st' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Quantizing trained pruned model ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m resnet18_quant_pr \u001b[38;5;241m=\u001b[39m quantize_trained_pruned_model(\u001b[43mresnet18_pruned_st\u001b[49m, train_loader)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#evaluate(resnet18_quant_pr, test_loader, quantization=True)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_size_mb(resnet18_quant_pr, use_state_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resnet18_pruned_st' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=== Quantizing trained pruned model ===\")\n",
    "resnet18_quant_pr = quantize_trained_pruned_model(resnet18_pruned_st, train_loader)\n",
    "#evaluate(resnet18_quant_pr, test_loader, quantization=True)\n",
    "model_size_mb(resnet18_quant_pr, use_state_dict=False)\n",
    "#measure_inference_speed(resnet18_quant_pr, test_loader, quantization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
