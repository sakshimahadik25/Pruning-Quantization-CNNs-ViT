{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssukuma2/.local/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/home/ssukuma2/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.quantization\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import timm\n",
    "import copy\n",
    "import os\n",
    "import tempfile\n",
    "from torch.ao.quantization import get_default_qconfig, prepare, convert\n",
    "from torch.ao.quantization.observer import MinMaxObserver, PerChannelMinMaxObserver\n",
    "from torch.ao.quantization.qconfig import QConfig\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/ssukuma2/.cache/kagglehub/datasets/msambare/fer2013/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"msambare/fer2013\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 22968\n",
      "Validation samples: 5741\n",
      "Test samples: 7178\n"
     ]
    }
   ],
   "source": [
    "# Dataset loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "dataset = ImageFolder('../FER2013/train', transform=transform)\n",
    "val_size = int(0.2 * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "test_dataset = ImageFolder('../FER2013/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Check distribution\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=7).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstructured Pruning for CNN\n",
    "def unstructured_prune_cnn(model, amount=0.3):\n",
    "    model = copy.deepcopy(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
    "            prune.remove(module, \"weight\")\n",
    "    return model\n",
    "\n",
    "# Structured Pruning for CNN\n",
    "def structured_prune_cnn(model, amount=0.5):\n",
    "    model = copy.deepcopy(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            prune.ln_structured(module, name=\"weight\", amount=amount, n=2, dim=0)\n",
    "            prune.remove(module, \"weight\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstructured Pruning for ViT\n",
    "def unstructured_prune_vit(model, amount=0.3):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
    "            prune.remove(module, \"weight\")\n",
    "    return model\n",
    "\n",
    "# Structured Attention Head Pruning for ViT\n",
    "def prune_vit_attention_heads(model, heads_to_prune=2):\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'qkv') and hasattr(module, 'num_heads'):\n",
    "            heads_dim = module.qkv.weight.shape[0] // 3\n",
    "            head_size = heads_dim // module.num_heads\n",
    "            qkv_weights = module.qkv.weight.data.view(3, module.num_heads, head_size, -1)\n",
    "            norms = qkv_weights.norm(dim=(2, 3))\n",
    "            importance = norms.sum(dim=0)\n",
    "            prune_indices = torch.topk(importance, heads_to_prune, largest=False).indices\n",
    "            for i in prune_indices:\n",
    "                qkv_weights[:, i, :, :] = 0\n",
    "            module.qkv.weight.data = qkv_weights.view(-1, module.qkv.weight.shape[1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_model_blocks(model):\n",
    "    torch.quantization.fuse_modules(model, [[\"conv1\", \"bn1\", \"relu\"]], inplace=True)\n",
    "    for module_name, module in model.named_children():\n",
    "        if \"layer\" in module_name:\n",
    "            for block in module:\n",
    "                torch.quantization.fuse_modules(\n",
    "                    block, [[\"conv1\", \"bn1\", \"relu\"], [\"conv2\", \"bn2\"]],\n",
    "                    inplace=True\n",
    "                )\n",
    "                if hasattr(block, \"downsample\") and isinstance(block.downsample, torch.nn.Sequential):\n",
    "                    if len(block.downsample) >= 2:\n",
    "                        torch.quantization.fuse_modules(block.downsample, [\"0\", \"1\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.ao.quantization.qconfig_mapping import get_default_qconfig_mapping\n",
    "\n",
    "# --- Custom quantization-safe LayerNorm replacement ---\n",
    "class QuantLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):\n",
    "        super().__init__()\n",
    "        if isinstance(normalized_shape, int):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        self.normalized_shape = tuple(normalized_shape)\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "        if self.elementwise_affine:\n",
    "            self.weight = nn.Parameter(torch.ones(*self.normalized_shape))\n",
    "            self.bias = nn.Parameter(torch.zeros(*self.normalized_shape))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, unbiased=False, keepdim=True)\n",
    "        x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        if self.elementwise_affine:\n",
    "            x = x * self.weight + self.bias\n",
    "        return x\n",
    "\n",
    "# --- Main function to quantize a pruned ViT model ---\n",
    "def quantize_pruned_vit_model(model, calibration_loader, num_batches=10):\n",
    "    import copy\n",
    "\n",
    "    # Clone the model to avoid modifying the original\n",
    "    model = copy.deepcopy(model)\n",
    "    model.eval().cpu()\n",
    "\n",
    "    # Replace incompatible modules (GELU, LayerNorm)\n",
    "    def patch_for_static_quant(model):\n",
    "        replacements = []\n",
    "\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.GELU):\n",
    "                replacements.append((name, nn.ReLU()))\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                try:\n",
    "                    qln = QuantLayerNorm(module.normalized_shape, eps=module.eps, elementwise_affine=True)\n",
    "                    qln.weight.data = module.weight.data.clone()\n",
    "                    qln.bias.data = module.bias.data.clone()\n",
    "                    replacements.append((name, qln))\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping {name} due to shape mismatch: {e}\")\n",
    "\n",
    "        # Apply replacements safely after iteration\n",
    "        for name, new_module in replacements:\n",
    "            parent = model\n",
    "            parts = name.split(\".\")\n",
    "            for part in parts[:-1]:\n",
    "                parent = getattr(parent, part)\n",
    "            setattr(parent, parts[-1], new_module)\n",
    "\n",
    "        return model\n",
    "\n",
    "    model = patch_for_static_quant(model)\n",
    "\n",
    "    # Quantization config\n",
    "    qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n",
    "\n",
    "    # Get example input for FX tracing\n",
    "    example_input = next(iter(calibration_loader))[0]\n",
    "\n",
    "    # Prepare for FX static quantization\n",
    "    prepared_model = prepare_fx(model, qconfig_mapping, example_inputs=example_input)\n",
    "\n",
    "    # Calibration loop\n",
    "    with torch.no_grad():\n",
    "        for i, (x, _) in enumerate(calibration_loader):\n",
    "            x = x.to(torch.float32)\n",
    "            prepared_model(x)\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "\n",
    "    # Convert to quantized model\n",
    "    quantized_model = convert_fx(prepared_model)\n",
    "    return quantized_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.ao.quantization.qconfig_mapping import get_default_qconfig_mapping\n",
    "\n",
    "\n",
    "def quantize_vit(model, calibration_loader, num_batches=10):\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "\n",
    "    qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n",
    "    example_input = next(iter(calibration_loader))[0]\n",
    "\n",
    "    # FX Graph Mode Quantization\n",
    "    prepared = prepare_fx(model, qconfig_mapping, example_inputs=example_input)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (x, _) in enumerate(calibration_loader):\n",
    "            prepared(x.to(torch.float32))\n",
    "            x.to(\"cpu\")\n",
    "            model(x)\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "\n",
    "    quantized_model = convert_fx(prepared)\n",
    "    return quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs=10, lr=1e-4):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, y)\n",
    "                val_loss += loss.item()\n",
    "                correct += (outputs.argmax(1) == y).sum().item()\n",
    "                total += y.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100 * correct / total\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, quantization=False):\n",
    "    model.eval()\n",
    "\n",
    "    if quantization:\n",
    "        model = model.to(\"cpu\")  # Quantized models must be on CPU\n",
    "    else:\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            if quantization:\n",
    "                x, y = x.to(\"cpu\"), y.to(\"cpu\")\n",
    "            else:\n",
    "                x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "\n",
    "            outputs = model(x)\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy = {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_speed(model, test_loader, quantization=False):\n",
    "    model.eval()\n",
    "    \n",
    "    if quantization:\n",
    "        device = \"cpu\"\n",
    "        \n",
    "    else:\n",
    "        device = torch.device(\"cuda\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for x, _ in test_loader:\n",
    "            x = x.to(device)\n",
    "            _ = model(x)\n",
    "    end = time.time()\n",
    "    latency = (end - start) / len(test_loader)\n",
    "    print(f\"Avg Inference Time per Batch: {latency:.4f} sec\")\n",
    "    return latency\n",
    "\n",
    "def model_size_mb(model, use_state_dict=True):\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "        if use_state_dict:\n",
    "            torch.save(model.state_dict(), f.name)\n",
    "        else:\n",
    "            torch.save(model, f.name)\n",
    "        size_mb = os.path.getsize(f.name) / (1024 * 1024)\n",
    "    print(f\"Model Size ({'state_dict' if use_state_dict else 'full model'}): {size_mb:.2f} MB\")\n",
    "    return size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Train baseline ===\n",
      "Epoch 1: Train Loss = 1.7604, Val Loss = 1.5743, Val Acc = 38.37%\n",
      "Test Accuracy = 40.40%\n",
      "Model Size (state_dict): 327.37 MB\n",
      "Avg Inference Time per Batch: 0.8452 sec\n",
      "=== Apply structured pruning, then fine-tune ===\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "prune_vit_attention_heads() got an unexpected keyword argument 'amount'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# === Apply structured pruning, then fine-tune ===\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Apply structured pruning, then fine-tune ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m vit_pruned_st \u001b[38;5;241m=\u001b[39m \u001b[43mprune_vit_attention_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m train(vit_pruned_st, train_loader, val_loader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n\u001b[1;32m     12\u001b[0m evaluate(vit_pruned_st, test_loader)\n",
      "\u001b[0;31mTypeError\u001b[0m: prune_vit_attention_heads() got an unexpected keyword argument 'amount'"
     ]
    }
   ],
   "source": [
    "# === Train baseline ===\n",
    "print(\"=== Train baseline ===\")\n",
    "train(vit, train_loader, val_loader, epochs=1)\n",
    "evaluate(vit, test_loader)\n",
    "model_size_mb(vit)\n",
    "measure_inference_speed(vit, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Apply structured pruning, then fine-tune ===\n",
      "Epoch 1: Train Loss = 1.4207, Val Loss = 1.3857, Val Acc = 46.84%\n",
      "Test Accuracy = 47.10%\n",
      "Model Size (state_dict): 327.37 MB\n",
      "Avg Inference Time per Batch: 0.8827 sec\n",
      "=== Apply unstructured pruning, then fine-tune ===\n",
      "Epoch 1: Train Loss = 1.3008, Val Loss = 1.2828, Val Acc = 51.38%\n",
      "Test Accuracy = 51.48%\n",
      "Model Size (state_dict): 327.37 MB\n",
      "Avg Inference Time per Batch: 0.8392 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8391690359706372"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Apply structured pruning, then fine-tune ===\n",
    "print(\"=== Apply structured pruning, then fine-tune ===\")\n",
    "vit_pruned_st = prune_vit_attention_heads(vit)\n",
    "train(vit_pruned_st, train_loader, val_loader, epochs=1, lr=1e-5)\n",
    "evaluate(vit_pruned_st, test_loader)\n",
    "model_size_mb(vit_pruned_st)\n",
    "measure_inference_speed(vit_pruned_st, test_loader)\n",
    "\n",
    "# === Apply unstructured pruning, then fine-tune ===\n",
    "print(\"=== Apply unstructured pruning, then fine-tune ===\")\n",
    "vit_pruned_unst = unstructured_prune_vit(vit, amount=0.5)\n",
    "train(vit_pruned_unst, train_loader, val_loader, epochs=1, lr=1e-5)\n",
    "evaluate(vit_pruned_unst, test_loader)\n",
    "model_size_mb(vit_pruned_unst)\n",
    "measure_inference_speed(vit_pruned_unst, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Apply quantization on baseline (no pruning) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssukuma2/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "<eval_with_key>.14:34: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:168.)\n",
      "  cat = torch.cat([quantize_per_tensor_6, patch_embed_norm], dim = 1);  quantize_per_tensor_6 = patch_embed_norm = None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 19.27%\n",
      "Model Size (state_dict): 84.06 MB\n",
      "Avg Inference Time per Batch: 2.3836 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.383622593584314"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Apply quantization on baseline (no pruning) ===\n",
    "print(\"=== Apply quantization on baseline (no pruning) ===\")\n",
    "vit_quant = quantize_vit(vit, train_loader)\n",
    "evaluate(vit_quant, test_loader, quantization=True)\n",
    "model_size_mb(vit_quant)\n",
    "measure_inference_speed(vit_quant, test_loader, quantization=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Apply quantization on pruned model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssukuma2/.local/lib/python3.10/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "<eval_with_key>.23:34: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:168.)\n",
      "  cat = torch.cat([quantize_per_tensor_6, patch_embed_norm], dim = 1);  quantize_per_tensor_6 = patch_embed_norm = None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 22.72%\n",
      "Model Size (state_dict): 84.12 MB\n",
      "Avg Inference Time per Batch: 1.7040 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.7040463654340896"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Apply quantization on pruned model ===\n",
    "print(\"=== Apply quantization on pruned model ===\")\n",
    "vit_quant_pr = quantize_pruned_vit_model(vit_pruned_st, train_loader)\n",
    "evaluate(vit_quant_pr, test_loader, quantization=True)\n",
    "model_size_mb(vit_quant_pr)\n",
    "measure_inference_speed(vit_quant_pr, test_loader, quantization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
